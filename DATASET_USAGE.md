# Dataset Usage Documentation

This document outlines how datasets are integrated, processed, and utilized within the BizBloom AI platform to provide intelligent insights, competitor analysis, and market trends.

## 1. Data Overview

BizBloom AI relies on a curated set of startup and business datasets to ground its AI generations in reality. Instead of halucinating competitors or trends, the system queries structured real-world data.

### Raw Data Sources
The raw datasets are located in `datasets/` and include:
- **`business-ideas-generated-with-gpt3.csv`**: A collection of AI-generated business concepts used for training and trend analysis.
- **`startup-success-prediction.csv`**: Historical data on startup performance.
- **`global-startup-success-dataset.csv`**: Comprehensive dataset of global startups, including their status (operating, acquired, closed).

## 2. Processing Pipeline

Before the application runs, raw data is transformed into efficient formats for real-time querying. The script `backend/scripts/process_datasets.py` handles this pipeline.

### Steps:
1.  **Ingestion & Merging**: Reads valid CSVs from `datasets/` and concatenates them into a unified dataframe.
2.  **Normalization**: Standardizes column names (e.g., mapping various description fields to a single `description` column) and extracts key fields: `name`, `description`, `industry`, `success_flag`.
3.  **Embedding Generation**: Uses the **Sentence-BERT (`all-MiniLM-L6-v2`)** model to convert startup descriptions into dense vector embeddings.
4.  **Index Creation**:
    *   **Vector Index**: Builds a nearest-neighbor index (e.g., FAISS or Annoy) to allow fast semantic similarity searches.
    *   **Metadata Storage**: Saves the normalized text data to `startup_metadata.csv` for retrieval after search.
    *   **Trend Extraction**: Aggregates industry-specific keywords and descriptions to create `trend_signals.csv`.

**Output**: The processed artifacts are saved to `datasets/processed/`.

## 3. Runtime Integration

The backend services load these processed files at startup to serve API requests.

### A. Competitor Analysis (`services/competitor_analysis.py`)
*   **Goal**: Identify existing startups similar to the user's idea.
*   **Mechanism**:
    1.  **Encoding**: The user's idea (Problem + Solution) is converted into a vector using the same embedding model.
    2.  **Vector Search**: The system queries the **startup index** to find the nearest vectors (most semantically similar descriptions).
    3.  **Retrieval**: It fetches the corresponding metadata (Name, Description, URL) for the top matches.
    4.  **Gap Analysis**: The system compares the retrieved competitors to the user's idea to suggest a "Market Gap."

### B. Market Insights (`services/market_insights.py`)
*   **Goal**: Provide industry context and trend analysis.
*   **Mechanism**:
    *   **Primary**: Uses LLM (OpenAI/OpenRouter) to generate context-aware insights.
    *   **Fallback/Augmentation**: Uses `trend_signals.csv` from the dataset.
        1.  **Keyword Matching**: Scans the user's idea for industry keywords (e.g., "education" -> EdTech).
        2.  **Trend Lookup**: Retrieves top trends associated with that industry from the pre-computed CSV.
        3.  **Customer Segmentation**: Maps the identified industry to standard customer profiles (e.g., "HealthTech" -> "Clinics & Patients").

## 4. Directory Structure

```plaintext
BizBloomAi/
├── datasets/
│   ├── business-ideas-generated-with-gpt3.csv
│   ├── startup-success-prediction.csv
│   ├── processed/                 <-- Generated by scripts
│   │   ├── startup_index.faiss    # Vector embeddings index
│   │   ├── startup_metadata.csv   # Normalized startup details
│   │   └── trend_signals.csv      # Industry trends lookup
└── backend/
    ├── scripts/
    │   └── process_datasets.py    # The ETL script
    └── app/
        └── services/
            ├── competitor_analysis.py  # Consumes vector index
            └── market_insights.py      # Consumes trend signals
```
